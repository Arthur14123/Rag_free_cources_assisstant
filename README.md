# Smart Search Tool for Free Courses

## Introduction
The **Smart Search Tool** is designed to provide users with the most relevant course recommendations related to **Data Science, Machine Learning and Gen AI** using **Retrival Augumented Generation** technology. By leveraging hybrid search mechanisms and the **Retrieval Augmented Generation (RAG)** approach, the tool helps users find precise and concise course results based on their queries.

The system combines **structured** and **unstructured data**, enabling an intuitive and efficient search experience. It utilizes **Pinecone** for vector database management, **LangChain** for orchestration, and **Groq's LLaMA-3.3-70B Versatile model** for natural language processing. The tool is deployed on **Hugging Face Spaces**, providing users easy access and interaction.

## Stack Design
The Smart Search Tool integrates several powerful technologies to deliver an optimized user experience:

- **Retrieval Augmented Generation (RAG)** for combining AI models with external knowledge bases.
- **Pinecone** for vector-based search and retrieval of course data.
- **LangChain** for orchestrating data flow and integrating various components.
- **Groq’s LLaMA-3.3-70B Versatile model** for generating high-quality responses.
- **Hugging Face’s all-MiniLM-L6-v2 model** for embedding generation in semantic search.
- **Streamlit** for building a user-friendly interface.
- **Hugging Face Spaces** for hosting and scaling the tool.

## Stack Implementation

### 1. Data Collection and Structuring
- **Source**: Course data was scraped from **Analytics Vidhya** platform's free course section.
- **Data**: Course titles, descriptions, topics, FAQs, instructor details, target audience, and access links.
- **Processing**: The unstructured data was converted into JSON format using **GPT-4**, which enabled easier processing and indexing.

### 2. Embedding Model and Vector Database Setup
- **Embedding Model**: Hugging Face’s **all-MiniLM-L6-v2** model was used to transform textual course data into embeddings for semantic search.
- **Vector Database**: **Pinecone** was used to store and index the embeddings for fast and scalable similarity searches.
- **Hybrid Search**: Combines dense semantic search using embeddings with sparse keyword-based search using BM25 for efficient and comprehensive search results.

### 3. Search Retriever Setup
A **Hybrid Search System** is implemented to merge:
- **Dense Search**: Uses embeddings generated by the **all-MiniLM-L6-v2** model for semantic matching.
- **Sparse Search**: Uses **BM25** encoding for keyword-based retrieval.

### 4. Query Handling and Response Generation
- **Query**: The user’s input is processed through the hybrid search system to retrieve relevant courses.
- **Response**: Responses are categorized into:
  - **General interactions**: Greetings or non-course-related queries.
  - **Course recommendations**: Displaying course titles, descriptions, and access links.
  - **Fallback responses**: Prompting users to explore more if no relevant courses are found.

### 5. Language Model and Embedding Model Selection
- **Embedding Model**: **HuggingFace’s all-MiniLM-L6-v2** was selected for its efficiency in semantic search tasks.
- **Language Model**: **LLaMA-3.3-70B Versatile** by **Groq** was used for natural language understanding and generating responses due to its fast, accurate inference capabilities.

### 6. LangChain for Orchestration
**LangChain** is utilized to orchestrate the flow of data between the components:
- **Prompt Templates**: Organized the user queries, context, and instructions into structured inputs to ensure accurate responses from the **LLaMA-3.3-70B** model.

### 7. Deployment and Interface
- **Deployment**: Hosted on **Hugging Face Spaces** for easy access and scalability.
- **Interface**: A simple chat-based UI is built using **Streamlit**, where users can interact with the search tool to get personalized course recommendations.

## Steps to Fork, Commit, and Create PR

1. **Fork the repository**:
   - Click on the "Fork" button at the top right of the repository page.
2. **Clone the repository**:
   ```bash
   git clone https://github.com/your-username/project-name.git
3. **Create a new branch**:
   ```bash
   git clone https://github.com/your-username/project-name.git
4. **Create a new branch**:
   ```bash
   git checkout -b feature-branch
5. **Make your changes**
6. **Commit your changes**:
   ```bash
   git add .
   git commit -m "Description of your changes"
7. **Push your changes**:
   ```bash
   git push origin feature-branch
8. **Create a Pull Request**:
   - Open a pull request on the original repository from your fork and describe your changes.
  
## Contributors
- [Sudarsanam Bharath](https://github.com/Bharath-tars)
- [Pooja Chinta](https://github.com/poojachinta24)

## References
- [Pinecone](https://www.pinecone.io)
- [LangChain](https://www.langchain.com)
- [LLaMA-3.3-70B](https://www.groq.com)
- [Hugging Face Spaces](https://huggingface.co/spaces)
- [Streamlit](https://streamlit.io)
- [Analytics Vidhya](https://www.analyticsvidhya.com)

## Technical Explanation of the Search Process
1. **User Query**: The user submits a query via the chat interface.
2. **Embedding Generation**: The query is encoded into a dense vector using **all-MiniLM-L6-v2**.
3. **Hybrid Search**: The system uses both dense search (Pinecone) and sparse search (BM25) to find the most relevant courses.
4. **Prompt Template Addition**: The relevant context and instructions are structured into a prompt by **LangChain**.
5. **Response Generation**: The structured prompt is passed to the **LLaMA-3.3-70B** model, which generates a human-like response.
6. **Response Display**: The response, including relevant courses and descriptions, is displayed to the user in the **Streamlit** UI.




